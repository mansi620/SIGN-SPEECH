{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install gdown","metadata":{"_uuid":"01ba17a6-1fc9-496b-9a26-7959c419326c","_cell_guid":"ca8842f5-18a9-4a06-83e6-f44ef5ed9e74","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-02-04T18:56:43.820194Z","iopub.execute_input":"2023-02-04T18:56:43.820573Z","iopub.status.idle":"2023-02-04T18:56:53.218155Z","shell.execute_reply.started":"2023-02-04T18:56:43.820529Z","shell.execute_reply":"2023-02-04T18:56:53.217035Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Requirement already satisfied: gdown in /opt/conda/lib/python3.7/site-packages (4.6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown) (3.7.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from gdown) (4.64.0)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.7/site-packages (from gdown) (2.28.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown) (1.15.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from gdown) (4.11.1)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->gdown) (2.3.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2022.12.7)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.26.14)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.7.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown --id 1f_c3uKTDQ4DR3CrwMSI8qdsTKJvKVt7p","metadata":{"execution":{"iopub.status.busy":"2023-02-04T18:56:53.220332Z","iopub.execute_input":"2023-02-04T18:56:53.220651Z","iopub.status.idle":"2023-02-04T18:56:56.008242Z","shell.execute_reply.started":"2023-02-04T18:56:53.220611Z","shell.execute_reply":"2023-02-04T18:56:56.007097Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.7/site-packages/gdown/cli.py:125: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=1f_c3uKTDQ4DR3CrwMSI8qdsTKJvKVt7p\nTo: /kaggle/working/hrnet_w48_coco_wholebody_384x288-6e061c6a_20200922.pth\n100%|█████████████████████████████████████████| 255M/255M [00:01<00:00, 245MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport regex as re\nimport pandas as pd\npath='/kaggle/input/adjective-trainer'\ndic={}\nlf=[]\ndf = pd.DataFrame(columns=['File','desc'])\nfor (root,dirs,files) in os.walk(path,topdown=True):\n    \n    l=[([i for i in files],root)]\n    lf.append(l)","metadata":{"execution":{"iopub.status.busy":"2023-02-04T18:56:56.011466Z","iopub.execute_input":"2023-02-04T18:56:56.011874Z","iopub.status.idle":"2023-02-04T18:56:56.083367Z","shell.execute_reply.started":"2023-02-04T18:56:56.011825Z","shell.execute_reply":"2023-02-04T18:56:56.082498Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"l=[]\nfor i in range(len(lf)) :\n    \n     for j in lf[i][0][0]:\n         l.append([j,lf[i][0][1]])","metadata":{"execution":{"iopub.status.busy":"2023-02-04T18:56:56.086507Z","iopub.execute_input":"2023-02-04T18:56:56.087341Z","iopub.status.idle":"2023-02-04T18:56:56.093775Z","shell.execute_reply.started":"2023-02-04T18:56:56.087306Z","shell.execute_reply":"2023-02-04T18:56:56.092827Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf=pd.DataFrame(l,columns=['file','root'])\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-04T18:56:56.095094Z","iopub.execute_input":"2023-02-04T18:56:56.095463Z","iopub.status.idle":"2023-02-04T18:56:56.108552Z","shell.execute_reply.started":"2023-02-04T18:56:56.095429Z","shell.execute_reply":"2023-02-04T18:56:56.107695Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"(791, 2)"},"metadata":{}}]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-04T18:56:56.112132Z","iopub.execute_input":"2023-02-04T18:56:56.114157Z","iopub.status.idle":"2023-02-04T18:56:56.125149Z","shell.execute_reply.started":"2023-02-04T18:56:56.114119Z","shell.execute_reply":"2023-02-04T18:56:56.124008Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"           file                                               root\n0  MVI_5295.MOV  /kaggle/input/adjective-trainer/Adjectives_6of...\n1  MVI_5138.MOV  /kaggle/input/adjective-trainer/Adjectives_6of...\n2  MVI_5297.MOV  /kaggle/input/adjective-trainer/Adjectives_6of...\n3  MVI_5136.MOV  /kaggle/input/adjective-trainer/Adjectives_6of...\n4  MVI_9408.MOV  /kaggle/input/adjective-trainer/Adjectives_6of...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file</th>\n      <th>root</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>MVI_5295.MOV</td>\n      <td>/kaggle/input/adjective-trainer/Adjectives_6of...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MVI_5138.MOV</td>\n      <td>/kaggle/input/adjective-trainer/Adjectives_6of...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MVI_5297.MOV</td>\n      <td>/kaggle/input/adjective-trainer/Adjectives_6of...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>MVI_5136.MOV</td>\n      <td>/kaggle/input/adjective-trainer/Adjectives_6of...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>MVI_9408.MOV</td>\n      <td>/kaggle/input/adjective-trainer/Adjectives_6of...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['full']=df['root']+'/'+df['file']\nx='extra'\ndfn=[]\nfor i in df.full:\n    if x not in i.lower().split('/'):\n        dfn.append(i)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-04T18:56:56.127106Z","iopub.execute_input":"2023-02-04T18:56:56.127981Z","iopub.status.idle":"2023-02-04T18:56:56.137654Z","shell.execute_reply.started":"2023-02-04T18:56:56.127945Z","shell.execute_reply":"2023-02-04T18:56:56.136604Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"len(df)","metadata":{"execution":{"iopub.status.busy":"2023-02-04T18:56:56.139242Z","iopub.execute_input":"2023-02-04T18:56:56.140033Z","iopub.status.idle":"2023-02-04T18:56:56.151482Z","shell.execute_reply.started":"2023-02-04T18:56:56.139998Z","shell.execute_reply":"2023-02-04T18:56:56.150383Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"791"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfn[0]","metadata":{"execution":{"iopub.status.busy":"2023-02-04T18:56:56.152743Z","iopub.execute_input":"2023-02-04T18:56:56.153658Z","iopub.status.idle":"2023-02-04T18:56:56.161786Z","shell.execute_reply.started":"2023-02-04T18:56:56.153507Z","shell.execute_reply":"2023-02-04T18:56:56.160721Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/adjective-trainer/Adjectives_6of8/Adjectives/87. hot/MVI_5295.MOV'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nimport argparse\nimport os\nimport pprint\n\nimport torch\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.nn.functional as f\n# import torch.optim\n# import torch.utils.data\n# import torch.utils.data.distributed\nimport torchvision.transforms as transforms\n\nsys.path.append('/kaggle/input/wholepose/wholepose')\nfrom pose_hrnet import get_pose_net\n# import coremltools as ct\nfrom collections import OrderedDict\nfrom config import cfg\nfrom config import update_config\n\nfrom PIL import Image\nimport numpy as np\nimport cv2\nfrom utils import pose_process, plot_pose\n\n\nmean = (0.485, 0.456, 0.406)\nstd = (0.229, 0.224, 0.225)\n\nindex_mirror = np.concatenate([\n                [1,3,2,5,4,7,6,9,8,11,10,13,12,15,14,17,16],\n                [21,22,23,18,19,20],\n                np.arange(40,23,-1), np.arange(50,40,-1),\n                np.arange(51,55), np.arange(59,54,-1),\n                [69,68,67,66,71,70], [63,62,61,60,65,64],\n                np.arange(78,71,-1), np.arange(83,78,-1),\n                [88,87,86,85,84,91,90,89],\n                np.arange(113,134), np.arange(92,113)\n                ]) - 1\nassert(index_mirror.shape[0] == 133)\n\nmulti_scales = [512,640]\ndef norm_numpy_totensor(img):\n    img = img.astype(np.float32) / 255.0\n    for i in range(3):\n        img[:, :, :, i] = (img[:, :, :, i] - mean[i]) / std[i]\n    return torch.from_numpy(img).permute(0, 3, 1, 2)\ndef stack_flip(img):\n    img_flip = cv2.flip(img, 1)\n    img_flip = torch.Tensor(img_flip)\n    return np.stack([img, img_flip], axis=0)\n\ndef merge_hm(hms_list):\n    assert isinstance(hms_list, list)\n    for hms in hms_list:\n        hms[1,:,:,:] = torch.flip(hms[1,index_mirror,:,:], [2])\n    \n    hm = torch.cat(hms_list, dim=0)\n    # print(hm.size(0))\n    hm = torch.mean(hms, dim=0)\n    return hm\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-02-04T18:56:56.165091Z","iopub.execute_input":"2023-02-04T18:56:56.165529Z","iopub.status.idle":"2023-02-04T18:56:56.180500Z","shell.execute_reply.started":"2023-02-04T18:56:56.165498Z","shell.execute_reply":"2023-02-04T18:56:56.179539Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n        config = '/kaggle/input/wholepose/wholepose/wholebody_w48_384x288.yaml'\n        cfg.merge_from_file(config)\n\n        # dump_input = torch.randn(1, 3, 256, 256)\n        # newmodel = PoseHighResolutionNet()\n        newmodel = get_pose_net(cfg, is_train=False)\n        #print(newmodel)\n        # dump_output = newmodel(dump_input)\n        # print(dump_output.size())\n        checkpoint = torch.load('/kaggle/working/hrnet_w48_coco_wholebody_384x288-6e061c6a_20200922.pth')\n        # newmodel.load_state_dict(checkpoint['state_dict'])\n\n\n        state_dict = checkpoint['state_dict']\n        new_state_dict = OrderedDict()\n        for k, v in state_dict.items():\n            if 'backbone.' in k:\n                name = k[9:] # remove module.\n            if 'keypoint_head.' in k:\n                name = k[14:] # remove module.\n            new_state_dict[name] = v\n        newmodel.load_state_dict(new_state_dict)\n\n        newmodel.cuda().eval()\n\n        transform  = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        ])\n\n      \n        \nfor i in dfn:\n     cap = cv2.VideoCapture(i)\n     frame_width = int(cap.get(3))\n     frame_height = int(cap.get(4))\n     output_list = []\n     while cap.isOpened():\n            success, img = cap.read()\n            if not success:\n                    print(\"Ignoring empty camera frame.\")\n                    # If loading a video, use 'break' instead of 'continue'.\n                    break\n            img = cv2.resize(img, (256,256))\n            frame_height, frame_width = img.shape[:2]\n            img = cv2.flip(img, flipCode=1)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            out = []\n            for scale in multi_scales:\n                    if scale != 512:\n                        img_temp = cv2.resize(img, (scale,scale))\n                    else:\n                        img_temp = img\n                    img_temp = stack_flip(img_temp)\n                    img_temp = norm_numpy_totensor(img_temp).cuda()\n                    hms = newmodel(img_temp)\n                    print(type(hms))\n                    if scale != 512:\n                        out.append(np.array(f.interpolate(hms, (frame_width // 4,frame_height // 4), mode='bilinear').tolist()))\n                    else:\n                        out.append(np.array(hms.tolist()))\n                    out = merge_hm(torch.from_numpy(out))\n                    result = out.reshape((133,-1))\n                    result = torch.argmax(result, dim=1)\n                    result = result.cpu().numpy().squeeze()\n                    y = result // (frame_width // 4)\n                    x = result % (frame_width // 4)\n                    pred = np.zeros((133, 3), dtype=np.float32)\n                    pred[:, 0] = x\n                    pred[:, 1] = y\n                    hm = out.cpu().detach().numpy().reshape((133, frame_height//4, frame_height//4))\n                    pred = pose_process(pred, hm)\n                    pred[:,:2] *= 4.0 \n                    print(pred.shape)\n                    assert pred.shape == (133, 3)\n                    output_list.append(pred)\n            output_list = np.array(output_list)\n            np.save(output_npy, output_list)\n            cap.release()\n                \n                    \n                    \n                    \n                    \n            \n            \n            \n            \n            \n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-02-04T19:04:57.537518Z","iopub.execute_input":"2023-02-04T19:04:57.537929Z","iopub.status.idle":"2023-02-04T19:04:59.393835Z","shell.execute_reply.started":"2023-02-04T19:04:57.537898Z","shell.execute_reply":"2023-02-04T19:04:59.392461Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"<class 'torch.Tensor'>\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24/3533349344.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                         \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_hm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m133\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got list)"],"ename":"TypeError","evalue":"expected np.ndarray (got list)","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}